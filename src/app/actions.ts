
"use server";
import { generateStartingPrompts } from '@/ai/flows/generate-starting-prompts';
import { summarizeModelResponse } from '@/ai/flows/summarize-model-response';
import { MODELS } from '@/lib/constants';
import type { IndividualResponse } from '@/lib/types';
import { ai } from '@/ai/genkit';
import { streamText } from 'genkit/ai';

async function getDummyResponse(prompt: string, modelName: string) {
  await new Promise(resolve => setTimeout(resolve, 1500 + Math.random() * 1000)); 
  return `This is a simulated response from **${modelName}** for the prompt: *"${prompt}"*.

ModelVerse is designed to be a clean and intuitive interface. Here are some of its features:

- **Model Selection**: Easily switch between models.
- **Unified Response**: Consistent output format.
- **Configuration**: Adjust parameters like temperature.

Here is a sample code block in JavaScript:
\`\`\`javascript
function greet(name) {
  // This function greets the user with the provided name.
  console.log(\`Hello, \${name}! Welcome to ModelVerse.\`);
}

greet('${modelName}');
\`\`\`

And a Python example:
\`\`\`python
def main():
    # Main function to demonstrate Python code
    model = "${modelName}"
    print(f"This response was generated by {model}.")

if __name__ == "__main__":
    main()
\`\`\`

The goal is to provide a seamless and powerful user experience.`;
}

export async function getSuggestions(modelName: string) {
  try {
    const result = await generateStartingPrompts({ modelName });
    return result.prompts;
  } catch (error) {
    console.error("Error fetching suggestions:", error);
    return [
        `What is the history of ${modelName}?`,
        `Write a short story in the style of a noir detective, where the main character is ${modelName}.`,
        `Explain the concept of neural networks like I'm five.`,
        `Generate a python script to parse a CSV file.`,
        `What are some creative uses for ${modelName}?`
    ];
  }
}

export async function processPrompt(prompt: string, modelName: string, apiKeys: Record<string, string>) {
  const modelInfo = MODELS.find(m => m.name === modelName);
  let response = '';

  try {
    if (modelInfo && apiKeys[modelInfo.id]) {
        const model = ai.model(modelInfo.id as any);
        const result = await ai.generate({ model, prompt, config: { apiKey: apiKeys[modelInfo.id] }});
        response = result.text;
    } else {
      response = await getDummyResponse(prompt, modelName);
    }
  } catch (error) {
    console.error(`Error with ${modelName}:`, error);
    response = `Error fetching response from ${modelName}. Falling back to dummy response.\n\n` + await getDummyResponse(prompt, modelName);
  }
  
  try {
    const summaryResult = await summarizeModelResponse({ modelResponse: response });
    return {
      response,
      summary: summaryResult.summary,
    };
  } catch(error) {
    console.error("Error summarizing response:", error);
    return {
        response,
        summary: "Could not generate summary for this response.",
    }
  }
}

const activeStreams: Map<string, AbortController> = new Map();

export async function processPromptStream(
    { prompt, modelName, apiKeys, flowId }: { prompt: string, modelName: string, apiKeys: Record<string, string>, flowId: string},
    onChunk: (chunk: string) => void
  ) {
    const modelInfo = MODELS.find(m => m.name === modelName);
    const abortController = new AbortController();
    activeStreams.set(flowId, abortController);
  
    try {
      if (modelInfo && apiKeys[modelInfo.id]) {
        const model = ai.model(modelInfo.id as any);
        const { stream } = await streamText({
          model,
          prompt,
          config: { apiKey: apiKeys[modelInfo.id] },
        });
        
        let fullResponse = '';
        for await (const chunk of stream) {
            if (abortController.signal.aborted) {
              console.log(`Stream ${flowId} aborted.`);
              break;
            }
            onChunk(chunk);
            fullResponse += chunk;
        }

        if (abortController.signal.aborted) {
            return null; // Don't proceed to summary if aborted
        }
  
        const summaryResult = await summarizeModelResponse({ modelResponse: fullResponse });
        return {
          response: fullResponse,
          summary: summaryResult.summary,
        };
  
      } else {
        const response = await getDummyResponse(prompt, modelName);
        // Simulate streaming for dummy response
        for (const word of response.split(' ')) {
            if (abortController.signal.aborted) break;
            onChunk(word + ' ');
            await new Promise(resolve => setTimeout(resolve, 50));
        }
        if (abortController.signal.aborted) return null;

        const summaryResult = await summarizeModelResponse({ modelResponse: response });
        return {
          response,
          summary: summaryResult.summary,
        };
      }
    } catch (error) {
      console.error(`Error with ${modelName}:`, error);
      const response = `Error fetching response from ${modelName}. Falling back to dummy response.\n\n` + await getDummyResponse(prompt, modelName);
      onChunk(response);
      return {
          response,
          summary: "Error generating summary."
      };
    } finally {
        activeStreams.delete(flowId);
    }
}

export async function stopGeneration(flowId: string) {
    if (activeStreams.has(flowId)) {
      activeStreams.get(flowId)?.abort();
      activeStreams.delete(flowId);
      console.log(`Requested to stop generation for flow: ${flowId}`);
    }
}
  

export async function processPromptAll(prompt: string, apiKeys: Record<string, string>): Promise<Omit<IndividualResponse, 'model'> & { model: { id: string; name: string; } }[]> {
  const allModelResponses = await Promise.all(
    MODELS.map(async (model) => {
      try {
        const result = await processPrompt(prompt, model.name, apiKeys);
        return {
          model: { id: model.id, name: model.name },
          ...result,
        };
      } catch (error) {
        console.error(`Error processing prompt for ${model.name}:`, error);
        const dummyResponse = await getDummyResponse(prompt, model.name);
        return {
          model: { id: model.id, name: model.name },
          response: `Error fetching response from ${model.name}.\n\n${dummyResponse}`,
          summary: "Error generating summary."
        };
      }
    })
  );

  return allModelResponses;
}
